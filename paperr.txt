Isolating Sources of Disentanglement in VAEs
Ricky T. Q. Chen, Xuechen Li, Roger Grosse, David Duvenaud
University of Toronto, Vector Institute
rtqichen, lxuechen, rgrosse, duvenaud@cs.toronto.edu
Abstract
We decompose the evidence lower bound to show the existence of a term measuring
the total correlation between latent variables. We use this to motivate the β-TCVAE
(Total Correlation Variational Autoencoder) algorithm, a reﬁnement and plug-in
replacement of the β-VAE for learning disentangled representations, requiring
no additional hyperparameters during training. We further propose a principled
classiﬁer-free measure of disentanglement called the mutual information gap (MIG).
We perform extensive quantitative and qualitative experiments, in both restricted
and non-restricted settings, and show a strong relation between total correlation
and disentanglement, when the model is trained using our framework.
1
Introduction
Learning disentangled representations without supervision is a difﬁcult open problem. Disentangled
variables are generally considered to contain interpretable semantic information and reﬂect separate
factors of variation in the data. While the deﬁnition of disentanglement is open to debate, many
believe a factorial representation, one with statistically independent variables, is a good starting
point [1, 2, 3]. Such representations distill information into a compact form which is oftentimes
semantically meaningful and useful for a variety of tasks [2, 4]. For instance, it is found that such
representations are more generalizable and robust against adversarial attacks [5].
Many state-of-the-art methods for learning disentangled representations are based on re-weighting
parts of an existing objective. For instance, it is claimed that mutual information between latent
variables and the observed data can encourage the latents into becoming more interpretable [6]. It
is also argued that encouraging independence between latent variables induces disentanglement
[7]. However, there is no strong evidence linking factorial representations to disentanglement. In
part, this can be attributed to weak qualitative evaluation procedures. While traversals in the latent
representation can qualitatively illustrate disentanglement, quantitative measures of disentanglement
are in their infancy.
In this paper, we:
• show a decomposition of the variational lower bound that can be used to explain the success
of the β-VAE [7] in learning disentangled representations.
• propose a simple method based on weighted minibatches to stochastically train with arbitrary
weights on the terms of our decomposition without any additional hyperparameters.
• introduce the β-TCVAE, which can be used as a plug-in replacement for the β-VAE with no
extra hyperparameters. Empirical evaluations suggest that the β-TCVAE discovers more
interpretable representations than existing methods, while also being fairly robust to random
initialization.
• propose a new information-theoretic disentanglement metric, which is classiﬁer-free and
generalizable to arbitrarily-distributed and non-scalar latent variables.
32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.
arXiv:1802.04942v5  [cs.LG]  23 Apr 2019
β-VAE [7]
β-TCVAE (Our)
(a) Baldness (-6, 6)
(b) Face width (0, 6)
(c) Gender (-6, 6)
(d) Mustache (-6, 0)
Figure 1: Qualitative comparisons on CelebA. Traversal ranges are shown in parentheses. Some
attributes are only manifested in one direction of a latent variable, so we show a one-sided traversal.
Most semantically similar variables from a β-VAE are shown for comparison.
While Kim & Mnih [8] have independently proposed augmenting VAEs with an equivalent total
correlation penalty to the β-TCVAE, their proposed training method differs from ours and requires
an auxiliary discriminator network.
2
Background: Learning and Evaluating Disentangled Representations
We discuss existing work that aims at either learning disentangled representations without supervision
or evaluating such representations. The two problems are inherently related, since improvements
to learning algorithms require evaluation metrics that are sensitive to subtle details, and stronger
evaluation metrics reveal deﬁciencies in existing methods.
2.1
Learning Disentangled Representations
VAE and β-VAE
The variational autoencoder (VAE) [9, 10] is a latent variable model that pairs a
top-down generator with a bottom-up inference network. Instead of directly performing maximum
likelihood estimation on the intractable marginal log-likelihood, training is done by optimizing the
tractable evidence lower bound (ELBO). We would like to optimize this lower bound averaged over
the empirical distribution (with β = 1):
Lβ = 1
N
N
X
n=1
(Eq[log p(xn|z)] −β KL (q(z|xn)||p(z)))
(1)
The β-VAE [7] is a variant of the variational autoencoder that attempts to learn a disentangled
representation by optimizing a heavily penalized objective with β > 1. Such simple penalization
has been shown to be capable of obtaining models with a high degree of disentanglement in image
datasets. However, it is not made explicit why penalizing KL(q(z|x)||p(z)) with a factorial prior can
lead to learning latent variables that exhibit disentangled transformations for all data samples.
InfoGAN
The InfoGAN [6] is a variant of the generative adversarial network (GAN) [11] that
encourages an interpretable latent representation by maximizing the mutual information between the
observation and a small subset of latent variables. The approach relies on optimizing a lower bound
of the intractable mutual information.
2.2
Evaluating Disentangled Representations
When the true underlying generative factors are known and we have reason to believe that this
set of factors is disentangled, it is possible to create a supervised evaluation metric. Many have
proposed classiﬁer-based metrics for assessing the quality of disentanglement [7, 8, 12, 13, 14, 15].
2
We focus on discussing the metrics proposed in [7] and [8], as they are relatively simple in design
and generalizable.
The Higgins’ metric [7] is deﬁned as the accuracy that a low VC-dimension linear classiﬁer can
achieve at identifying a ﬁxed ground truth factor. Speciﬁcally, for a set of ground truth factors
{vk}K
k=1, each training data point is an aggregation over L samples:
1
L
PL
l=1 |z(1)
l
−z(2)
l
|, where
random vectors z(1)
l
, z(2)
l
are drawn i.i.d. from q(z|vk)1 for any ﬁxed value of vk, and a classiﬁcation
target k. A drawback of this method is the lack of axis-alignment detection. That is, we believe a truly
disentangled model should only contain one latent variable that is related to each factor. As a means
to include axis-alignment detection, [8] proposes using argminj Varq(zj|vk)[zj] and a majority-vote
classiﬁer.
Classiﬁer-based disentanglement metrics tend to be ad-hoc and sensitive to hyperparameters. The
metrics in [7] and [8] can be loosely interpreted as measuring the reduction in entropy of z if v
is observed. In section 4, we show that it is possible to directly measure the mutual information
between z and v which is a principled information-theoretic quantity that can be used for any latent
distributions provided that efﬁcient estimation exists.
3
Sources of Disentanglement in the ELBO
It is suggested that two quantities are especially important in learning a disentangled representation
[6, 7]: A) Mutual information between the latent variables and the data variable, and B) Independence
between the latent variables.
A term that quantiﬁes criterion A was illustrated by an ELBO decomposition [16]. In this section, we
introduce a reﬁned decomposition showing that terms describing both criteria appear in the ELBO.
ELBO TC-Decomposition
We identify each training example with a unique integer index and
deﬁne a uniform random variable on {1, 2, ..., N} with which we relate to data points.
Fur-
thermore, we deﬁne q(z|n) = q(z|xn) and q(z, n) = q(z|n)p(n) = q(z|n) 1
N . We refer to
q(z) = PN
n=1 q(z|n)p(n) as the aggregated posterior following [17], which captures the aggregate
structure of the latent variables under the data distribution. With this notation, we decompose the KL
term in (1) assuming a factorized p(z).
Ep(n)

KL
 q(z|n)||p(z)

= KL (q(z, n)||q(z)p(n))
|
{z
}
i
Index-Code MI
+ KL
 q(z)||
Y
j
q(zj)

|
{z
}
ii
Total Correlation
+
X
j
KL (q(zj)||p(zj))
|
{z
}
iii Dimension-wise KL
(2)
where zj denotes the jth dimension of the latent variable.
Decomposition Analysis
In a similar decomposition [16],
i
is referred to as the index-code
mutual information (MI). The index-code MI is the mutual information Iq(z; n) between the data
variable and latent variable based on the empirical data distribution q(z, n). It is argued that a
higher mutual information can lead to better disentanglement [6], and some have even proposed to
completely drop the penalty on this term during optimization [17, 18]. However, recent investigations
into generative modeling also claim that a penalized mutual information through the information
bottleneck encourages compact and disentangled representations [3, 19].
In information theory, ii is referred to as the total correlation (TC), one of many generalizations
of mutual information to more than two random variables [20]. The naming is unfortunate as it is
actually a measure of dependence between the variables. The penalty on TC forces the model to ﬁnd
statistically independent factors in the data distribution. We claim that a heavier penalty on this term
induces a more disentangled representation, and that the existence of this term is the reason β-VAE
has been successful.
1Note that q(z|vk) is sampled by using an intermediate data sample: z ∼q(z|x), x ∼p(x|vk).
3
We refer to iii as the dimension-wise KL. This term mainly prevents individual latent dimensions
from deviating too far from their corresponding priors. It acts as a complexity penalty on the aggregate
posterior which reasonably follows from the minimum description length [21] formulation of the
ELBO.
We would like to verify the claim that TC is the most important term in this decomposition for
learning disentangled representations by penalizing only this term; however, it is difﬁcult to estimate
the three terms in the decomposition. In the following section, we propose a simple yet general
framework for training with the TC-decomposition using minibatches of data.
A special case of this decomposition was given in [22], assuming that the use of a ﬂexible prior can
effectively ignore the dimension-wise KL term. In contrast, our decomposition (2) is more generally
applicable to many applications of the ELBO.
3.1
Training with Minibatch-Weighted Sampling
We describe a method to stochastically estimate the decomposition terms, allowing scalable training
with arbitrary weights on each decomposition term. Note that the decomposed expression (2) requires
the evaluation of the density q(z) = Ep(n)[q(z|n)], which depends on the entire dataset2. As such, it
is undesirable to compute it exactly during training. One main advantage of our stochastic estimation
method is the lack of hyperparameters or inner optimization loops, which should provide more stable
training.
A naïve Monte Carlo approximation based on a minibatch of samples from p(n) is likely to underes-
timate q(z). This can be intuitively seen by viewing q(z) as a mixture distribution where the data
index n indicates the mixture component. With a randomly sampled component, q(z|n) is close to 0,
whereas q(z|n) would be large if n is the component that z came from. So it is much better to sample
this component and weight the probability appropriately.
To this end, we propose using a weighted version for estimating the function log q(z) during training,
inspired by importance sampling. When provided with a minibatch of samples {n1, ..., nM}, we can
use the estimator
Eq(z)[log q(z)] ≈1
M
M
X
i=1

log
1
NM
M
X
j=1
q(z(ni)|nj)


(3)
where z(ni) is a sample from q(z|ni) (see derivation in Appendix C). This minibatch estimator is
biased, since its expectation is a lower bound3. However, computing it does not require any additional
hyperparameters.
3.1.1
Special case: β-TCVAE
With minibatch-weighted sampling, it is easy to assign different weights (α, β, γ) to the terms in (2):
Lβ−TC := Eq(z|n)p(n)[log p(n|z)] −αIq(z; n) −β KL
 q(z)||
Y
j
q(zj)

−γ
X
j
KL (q(zj)||p(zj))
(4)
While we performed ablation experiments with different values for α and γ, we ultimately ﬁnd that
tuning β leads to the best results. Our proposed β-TCVAE uses α = γ = 1 and only modiﬁes the
hyperparameter β. While Kim & Mnih [8] have proposed an equivalent objective, they estimate TC
using an auxiliary discriminator network.
4
Measuring Disentanglement with the Mutual Information Gap
It is difﬁcult to compare disentangling algorithms without a proper metric. Most prior work has
resorted to qualitative analysis by visualizing the latent representation. Another approach relies
on knowing the true generative process p(n|v) and ground truth latent factors v. Often these are
2The same argument holds for the term Q
j q(zj) and a similar estimator can be constructed.
3This follows from Jensen’s inequality Ep(n)[log q(z|n)] ≤log Ep(n)[q(z|n)].
4
semantically meaningful attributes of the data. For instance, photographic portraits generally contain
disentangled factors such as pose (azimuth and elevation), lighting condition, and attributes of the
face such as skin tone, gender, face width, etc. Though not all ground truth factors may be provided,
it is still possible to evaluate disentanglement using the known factors. We propose a metric based on
the empirical mutual information between latent variables and ground truth factors.
4.1
Mutual Information Gap (MIG)
Our key insight is that the empirical mutual information between a latent variable zj and a
ground truth factor vk can be estimated using the joint distribution deﬁned by q(zj, vk) =
PN
n=1 p(vk)p(n|vk)q(zj|n). Assuming that the underlying factors p(vk) and the generating process
is known for the empirical data samples p(n|vk), then
In(zj; vk) = Eq(zj,vk)

log
X
n∈Xvk
q(zj|n)p(n|vk)

+ H(zj)
(5)
where Xvk is the support of p(n|vk). (See derivation in Appendix B.)
A higher mutual information implies that zj contains a lot of information about vk, and the mutual
information is maximal if there exists a deterministic, invertible relationship between zj and vj.
Furthermore, for discrete vk, 0 ≤I(zj; vk) ≤H(vk), where H(vk) = Ep(vk)[−log p(vk)] is the
entropy of vk. As such, we use the normalized mutual information I(zj; vk)/H(vk).
Note that a single factor can have high mutual information with multiple latent variables. We enforce
axis-alignment by measuring the difference between the top two latent variables with highest mutual
information. The full metric we call mutual information gap (MIG) is then
1
K
K
X
k=1
1
H(vk)

In(zj(k); vk) −max
j̸=j(k) In(zj; vk)

(6)
where j(k) = argmaxj In(zj; vk) and K is the number of known factors. MIG is bounded by 0 and 1.
We perform an entire pass through the dataset to estimate MIG.
While it is possible to compute just the average maximal MI,
1
K
PK
k=1
In(zk∗;vk)
H(vk)
, the gap in our
formulation (6) defends against two important cases. The ﬁrst case is related to rotation of the factors.
When a set of latent variables are not axis-aligned, each variable can contain a decent amount of
information regarding two or more factors. The gap heavily penalizes unaligned variables, which is
an indication of entanglement. The second case is related to compactness of the representation. If one
latent variable reliably models a ground truth factor, then it is unnecessary for other latent variables
to also be informative about this factor.
Metric
Axis
Unbiased
General
Higgins et al. [7]
No
No
No
Kim & Mnih [8]
Yes
No
No
MIG (Ours)
Yes
Yes
Yes
Table 1: In comparison to prior metrics, our proposed
MIG detects axis-alignment, is unbiased for all hyper-
parameter settings, and can be generally applied to any
latent distributions provided efﬁcient estimation exists.
As summarized in Table 1, our metric de-
tects axis-alignment and is generally appli-
cable and meaningful for any factorized la-
tent distribution, including vectors of mul-
timodal, categorical, and other structured
distributions. This is because the metric is
only limited by whether the mutual informa-
tion can be estimated. Efﬁcient estimation of
mutual information is an ongoing research
topic [23, 24], but we ﬁnd that the simple
estimator (5) can be computed within rea-
sonable amount of time for the datasets we
use. We ﬁnd that MIG can better capture
subtle differences in models compared to existing metrics. Systematic experiments analyzing MIG
and existing metrics are in Appendix G.
5
Related Work
We focus on discussing the learning of disentangled representations in an unsupervised manner.
Nevertheless, we note that inverting generative processes with known disentangled factors through
5
weak supervision has been pursued by many. The goal in this case is not perfect inversion but to
distill simpler representation [15, 25, 26, 27, 28]. Although not explicitly the main motivation, many
unsupervised generative modeling frameworks have explored the disentanglement of their learned
representations [9, 17, 29]. Prior to β-VAE [7], some have shown successful disentanglement in
limited settings with few factors of variation [1, 14, 30, 31].
As a means to describe the properties of disentangled representations, factorial representations have
been motivated by many [1, 2, 3, 22, 32, 33]. In particular, Appendix B of [22] shows the existence of
the total correlation in a similar objective with a ﬂexible prior and assuming optimality q(z) = p(z).
Similarly, [34] arrives at the ELBO from an objective that combines informativeness and the total
correlation of latent variables. In contrast, we show a more general analysis of the unmodiﬁed
evidence lower bound.
The existence of the index-code MI in the ELBO has been shown before [16], and as a result,
FactorVAE, which uses an equivalent objective to the β-TCVAE, is independently proposed [8].
The main difference is they estimate the total correlation using the density ratio trick [35] which
requires an auxiliary discriminator network and an inner optimization loop. In contrast, we emphasize
the success of β-VAE using our reﬁned decomposition, and propose a training method that allows
assigning arbitrary weights to each term of the objective without requiring any additional networks.
In a similar vein, non-linear independent component analysis [36, 37, 38] studies the problem of
inverting a generative process assuming independent latent factors. Instead of a perfect inversion, we
only aim for maximizing the mutual information between our learned representation and the ground
truth factors. Simple priors can further encourage interpretability by means of warping complex
factors into simpler manifolds. To the best of our knowledge, we are the ﬁrst to show a strong
quantiﬁable relation between factorial representations and disentanglement (see Section 6).
6
Experiments
Dataset
Ground truth factors
dSprites
scale (6), rotation (40), posX (32), posY (32)
3D Faces
azimuth (21), elevation (11), lighting (11)
Table 2: Summary of datasets with known ground truth fac-
tors. Parentheses contain number of quantized values for each
factor.
We perform a series of quantitative
and qualitative experiments, show-
ing that β-TCVAE can consistently
achieve higher MIG scores compared
to prior methods β-VAE [7] and Info-
GAN [6], and can match the perfor-
mance of FactorVAE [8] whilst per-
forming better in scenarios where the
density ratio trick is difﬁcult to train.
Furthermore, we ﬁnd that in models
trained with our method, total corre-
lation is strongly correlated with disentanglement.4
Independent Factors of Variation
First, we analyze the performance of our proposed β-TCVAE
and MIG metric in a restricted setting, with ground truth factors that are uniformly and independently
sampled. To paint a clearer picture on the robustness of learning algorithms, we aggregate results
from multiple experiments to visualize the effect of initialization .
We perform quantitative evaluations with two datasets, a dataset of 2D shapes [39] and a dataset of
synthetic 3D faces [40]. Their ground truth factors are summarized in Table 2. The dSprites and 3D
faces also contain 3 types of shapes and 50 identities, respectively, which are treated as noise during
evaluation.
ELBO vs. Disentanglement Trade-off
Since the β-VAE and β-TCVAE objectives are lower
bounds on the standard ELBO, we would like to see the effect of training with this modiﬁcation.
To see how the choice of β affects these learning algorithms, we train using a range of values. The
trade-off between density estimation and the amount of disentanglement measured by MIG is shown
in Figure 2.
4Code is available at https://github.com/rtqichen/beta-tcvae.
6
2
4
6
8
10
160
140
120
100
80
60
40
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
MIG Metric
-VAE
-TCVAE
-TCVAE (
= 0)
-TCVAE (FacFlow)
(a) dSprites
2
4
6
8
10
1450
1440
1430
1420
1410
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
0.5
0.6
MIG Metric
-VAE
-TCVAE
-TCVAE (
= 0)
-TCVAE (FacFlow)
(b) 3D Faces
Figure 2: Compared to β-VAE, β-TCVAE creates more disentangled representations while preserving
a better generative model of the data with increasing β. Shaded regions show the 90% conﬁdence
intervals. Higher is better on both metrics.
VAE
InfoGAN
-VAE
FactorVAE
-TCVAE
0.0
0.2
0.4
0.6
Disentanglement (MIG)
Disentanglement on dSprites
(a) dSprites
VAE
InfoGAN
-VAE
FactorVAE
-TCVAE
0.0
0.2
0.4
0.6
Disentanglement (MIG)
Disentanglement on 3D Faces
(b) 3D Faces
Figure 3: Distribution of disentanglement score
(MIG) for different modeling algorithms.
10-1
100
Total correlation
0.05
0.10
0.15
0.20
0.25
0.30
0.35
MIG
β-TCVAE
ρ=-0.89
β-VAE
ρ=-0.72
(a) dSprites
10-1
100
Total correlation
0.2
0.3
0.4
0.5
0.6
MIG
β-TCVAE
ρ=-0.99
β-VAE
ρ=-0.93
(b) 3D Faces
Figure 4: Scatter plots of the average MIG and TC
per value of β. Larger circles indicate a higher β.
We ﬁnd that β-TCVAE provides a better trade-off between density estimation and disentanglement.
Notably, with higher values of β, the mutual information penality in β-VAE is too strong and this
hinders the usefulness of the latent variables. However, β-TCVAE with higher values of β consistently
results in models with higher disentanglement score relative to β-VAE.
We also perform ablation studies on the removal of the index-code MI term by setting α = 0
in (4), and a model using a factorized normalizing ﬂow as the prior distribution which is jointly
trained to maximize the modiﬁed objective. Neither resulted in signiﬁcant performance difference,
suggesting that tuning the weight of the TC term in (2) is the most useful for learning disentangled
representations.
Quantitative Comparisons
While a disentangled representation may be achievable by some learn-
ing algorithms, the chances of obtaining such a representation typically is not clear. Unsupervised
learning of a disentangled representation can have high variance since disentangled labels are not
provided during training. To further understand the robustness of each algorithm, we show box
plots depicting the quartiles of the MIG score distribution for various methods in Figure 3. We
used β = 4 for β-VAE and β = 6 for β-TCVAE, based on modes in Figure 2. For InfoGAN, we
used 5 continuous latent codes and 5 noise variables. Other settings are chosen following those
suggested by [6], but we also added instance noise [41] to stabilize training. FactorVAE uses an
equivalent objective to the β-TCVAE but is trained with the density ratio trick [35], which is known
to underestimate the TC term [8]. As a result, we tuned β ∈[1, 80] and used double the number of
iterations for FactorVAE. Note that while β-VAE, FactorVAE and β-TCVAE use a fully connected
architecture for the dSprites dataset, InfoGAN uses a convolutional architecture for increased stability.
We also ﬁnd that FactorVAE performs poorly with fully connected layers, resulting in worse results
than β-VAE on the dSprites dataset.
In general, we ﬁnd that the median score is highest for β-TCVAE and it is close to the highest score
achieved by all methods. Despite the best half of the β-TCVAE runs achieving relatively high scores,
we see that the other half can still perform poorly. Low-score outliers exist in the 3D faces dataset,
although their scores are still higher than the median scores achieved by both VAE and InfoGAN.
Factorial vs. Disentangled Representations
While a low total correlation has been previously
conjectured to lead to disentanglement, we provide concrete evidence that our β-TCVAE learning
algorithm satisﬁes this property. Figure 4 shows a scatter plot of total correlation and the MIG
7
azimuth
elevation
A
azimuth
elevation
B
azimuth
elevation
C
azimuth
elevation
D
(a) Different joint
distributions of factors.
A
B
C
D
0.0
0.2
0.4
0.6
0.8
1.0
Disentanglement (MIG)
Robustness to Sampling Bias
InfoGAN
-VAE
-TCVAE
(b) Distribution of disentanglement scores (MIG).
Figure 5: The β-TCVAE has a higher chance of obtaining a disentangled representation than β-
VAE, even in the presence of sampling bias. (a) All samples have non-zero probability in all joint
distributions; the most likely sample is 4 times as likely as the least likely sample.
disentanglement metric for varying values of β trained on the dSprites and faces datasets, averaged
over 40 random initializations. For models trained with β-TCVAE, the correlation between average
TC and average MIG is strongly negative, while models trained with β-VAE have a weaker correlation.
In general, for the same degree of total correlation, β-TCVAE creates a better disentangled model.
This is also strong evidence for the hypothesis that large values of β can be useful as long as the
index-code mutual information is not penalized.
6.1
Correlated or Dependent Factors
A notion of disentanglement can exist even when the underlying generative process samples factors
non-uniformly and dependently sampled. Many real datasets exhibit this behavior, where some con-
ﬁgurations of factors are sampled more than others, violating the statistical independence assumption.
Disentangling the factors of variation in this case corresponds to ﬁnding the generative model where
the latent factors can independently act and perturb the generated result, even when there is bias in
the sampling procedure. In general, we ﬁnd that β-TCVAE has no problem in ﬁnding the correct
factors of variation in a toy dataset and can ﬁnd more interpretable factors of variation than those
found in prior work, even though the independence assumption is violated.
Two Factors
We start off with a toy dataset with only two factors and test β-TCVAE using
sampling distributions with varying degrees of correlation and dependence. We take the dataset of
synthetic 3D faces and ﬁx all factors other than pose. The joint distributions over factors that we test
with are summarized in Figure 5a, which includes varying degrees of sampling bias. Speciﬁcally,
conﬁguration A uses uniform and independent factors; B uses factors with non-uniform marginals but
are uncorrelated and independent; C uses uncorrelated but dependent factors; and D uses correlated
and dependent factors. While it is possible to train a disentangled model in all conﬁgurations, the
chances of obtaining one is overall lower when there exist sampling bias. Across all conﬁgurations,
we see that β-TCVAE is superior to β-VAE and InfoGAN, and there is a large difference in median
scores for most conﬁgurations.
6.1.1
Qualitative Comparisons
We show qualitatively that β-TCVAE discovers more disentangled factors than β-VAE on datasets of
chairs [42] and real faces [43].
3D Chairs
Figure 6 shows traversals in latent variables that depict an interpretable property in
generating 3D chairs. The β-VAE [7] has shown to be capable of learning the ﬁrst four properties:
azimuth, size, leg style, and backrest. However, the leg style change learned by β-VAE does not seem
to be consistent for all chairs. We ﬁnd that β-TCVAE can learn two additional interpretable properties:
material of the chair, and leg rotation for swivel chairs. These two properties are more subtle and
likely require a higher index-code mutual information, so the lower penalization of index-code MI in
β-TCVAE helps in ﬁnding these properties.
8
β-VAE
β-TCVAE
(a) Azimuth
(b) Chair size
(c) Leg style
(d) Backrest
(e) Material
(f) Swivel
Figure 6: Learned latent variables using β-VAE and β-TCVAE are shown. Traversal range is (-2, 2).
CelebA
Figure 1 shows 4 out of 15 attributes that are discovered by the β-TCVAE without super-
vision (see Appendix A.3). We traverse up to six standard deviations away from the mean to show
the effect of generalizing the represented semantics of each variable. The representation learned
by β-VAE is entangled with nuances, which can be shown when generalizing to low probability
regions. For instance, it has difﬁculty rendering complete baldness or narrow face width, whereas the
β-TCVAE shows meaningful extrapolation. The extrapolation of the gender attribute of β-TCVAE
shows that it focuses more on gender-speciﬁc facial features, whereas the β-VAE is entangled with
many irrelevances such as face width. The ability to generalize beyond the ﬁrst few standard devia-
tions of the prior mean implies that the β-TCVAE model can generate rare samples such as bald or
mustached females.
7
Conclusion
We present a decomposition of the ELBO with the goal of explaining why β-VAE works. In particular,
we ﬁnd that a TC penalty in the objective encourages the model to ﬁnd statistically independent
factors in the data distribution. We then designate a special case as β-TCVAE, which can be trained
stochastically using minibatch estimator with no additional hyperparameters compared to the β-VAE.
The simplicity of our method allows easy integration into different frameworks [44]. To quantitatively
evaluate our approach, we propose a classiﬁer-free disentanglement metric called MIG. This metric
beneﬁts from advances in efﬁcient computation of mutual information [23] and enforces compactness
in addition to disentanglement. Unsupervised learning of disentangled representations is inherently
a difﬁcult problem due to the lack of a prior for semantic awareness, but we show some evidence
in simple datasets with uniform factors that independence between latent variables can be strongly
related to disentanglement.
Acknowledgements
We thank Alireza Makhzani, Yuxing Zhang, and Bowen Xu for initial discussions. We also thank
Chatavut Viriyasuthee for pointing out an error in one of our derivations. Ricky would also like to
thank Brendan Shillingford for supplying accommodation at a popular conference.
References
[1] Jürgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation,
4(6):863–879, 1992.
[2] Karl Ridgeway.
A survey of inductive biases for factorial representation-learning.
arXiv preprint
arXiv:1612.05299, 2016.
[3] Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in deep
representations. arXiv preprint arXiv:1706.01350, 2017.
[4] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.
9
[5] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. International Conference on Learning Representations, 2017.
[6] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.
Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In Advances
in Neural Information Processing Systems, pages 2172–2180, 2016.
[7] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational
framework. International Conference on Learning Representations, 2017.
[8] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.
[9] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,
2013.
[10] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-
mate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672–2680, 2014.
[12] Will Grathwohl and Aaron Wilson. Disentangling space and time in video with hierarchical variational
auto-encoders. arXiv preprint arXiv:1612.04440, 2016.
[13] Christopher K. I. Williams Cian Eastwood. A framework for the quantitative evaluation of disentangled
representations. International Conference on Learning Representations, 2018.
[14] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment clas-
siﬁcation: A deep learning approach. In Proceedings of the 28th international conference on machine
learning (ICML-11), pages 513–520, 2011.
[15] Theofanis Karaletsos, Serge Belongie, and Gunnar Rätsch. Bayesian representation learning with oracle
constraints. International Conference on Learning Representations, 2016.
[16] Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the variational
evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016.
[17] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. ICLR 2016 Workshop, International Conference on Learning Representations, 2016.
[18] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational autoen-
coders. arXiv preprint arXiv:1706.02262, 2017.
[19] Christopher Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and
Alexander Lerchner. Understanding disentangling in beta-vae. Learning Disentangled Representations:
From Perception to Control Workshop, 2017.
[20] Satosi Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and
development, 4(1):66–82, 1960.
[21] Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length and helmholtz free
energy. In Advances in neural information processing systems, pages 3–10, 1994.
[22] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through
noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.
[23] Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon Hjelm, and Aaron Courville. MINE: Mutual
information neural estimation. arXiv preprint arXiv:1801.04062, 2018.
[24] David N Reshef, Yakir A Reshef, Hilary K Finucane, Sharon R Grossman, Gilean McVean, Peter J
Turnbaugh, Eric S Lander, Michael Mitzenmacher, and Pardis C Sabeti. Detecting novel associations in
large data sets. science, 334(6062):1518–1524, 2011.
[25] Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In International
Conference on Artiﬁcial Neural Networks, pages 44–51. Springer, 2011.
[26] Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse
graphics network. In Advances in Neural Information Processing Systems, pages 2539–2547, 2015.
[27] Brian Cheung, Jesse A Livezey, Arjun K Bansal, and Bruno A Olshausen. Discovering hidden factors of
variation in deep networks. arXiv preprint arXiv:1412.6583, 2014.
[28] Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of visually
grounded imagination. International Conference on Learning Representations, 2018.
[29] Alec Radford, Luke Metz, and Soumith Chintala.
Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
10
[30] Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton. Tensor analyzers. In International Conference
on Machine Learning, pages 163–171, 2013.
[31] Guillaume Desjardins, Aaron Courville, and Yoshua Bengio. Disentangling factors of variation via
generative entangling. arXiv preprint arXiv:1210.5474, 2012.
[32] Greg Ver Steeg and Aram Galstyan.
Maximally informative hierarchical representations of high-
dimensional data. In Artiﬁcial Intelligence and Statistics, pages 1004–1012, 2015.
[33] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled
latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.
[34] Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Auto-encoding total correlation
explanation. arXiv preprint arXiv:1802.05822, 2018.
[35] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine learning.
Cambridge University Press, 2012.
[36] Pierre Comon. Independent component analysis, a new concept? Signal processing, 36(3):287–314, 1994.
[37] Aapo Hyvärinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness
results. Neural Networks, 12(3):429–439, 1999.
[38] Christian Jutten and Juha Karhunen. Advances in nonlinear blind source separation. In Proc. of the 4th Int.
Symp. on Independent Component Analysis and Blind Signal Separation, 2003.
[39] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing
sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
[40] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model for
pose and illumination invariant face recognition. In Advanced Video and Signal Based Surveillance, 2009.
AVSS’09. Sixth IEEE International Conference on, pages 296–301. Ieee, 2009.
[41] Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised map
inference for image super-resolution. International Conference on Learning Representations, 2017.
[42] Mathieu Aubry, Daniel Maturana, Alexei Efros, Bryan Russell, and Josef Sivic. Seeing 3d chairs: exemplar
part-based 2d-3d alignment using a large dataset of cad models. In CVPR, 2014.
[43] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE International Conference on Computer Vision, pages 3730–3738, 2015.
[44] Jin Xu and Yee Whye Teh. Controllable semantic image inpainting. arXiv preprint arXiv:1806.05953,
2018.
11
Appendix for Isolating Sources of Disentanglement in
Variational Autoencoders
A. Random Samples
A.1 Qualitative Samples
dSprites (64 × 64)
3D Faces (64 × 64)
3D Chairs (64 × 64)
CelebA (64 × 64)
Figure S1: Real samples from the training data set.
1
A.3 CelebA Latent Traversals
β-TCVAE Model One (β=15)
Baldness
Dramatic masculinity
Azimuth
2
Contrast
Mustache (shared with Glasses)
Glasses (shared with Mustache)
3
Smile (shared with Shadow)
Shadow (shared with Smile)
Gender
4
Skin color
Brightness
Bangs (side)
5
Hue
Face width
Eye shadow
6
B. Mutual Information Gap
B.1 Estimation of I(zk; vk)
With any inference network q(z|x), we can compute the mutual information I(z; v) by assuming
the model p(v)p(x|v)q(z|x). Speciﬁcally, we compute this for every pair of latent variable zj and
ground truth factor vk.
We make the following assumptions:
• The inference distribution q(zj|x) can be sampled from and is known for all j.
• The generating process p(n|vk) can be sampled from and is known.
• Simplifying assumption: p(vk) and p(n|vk) are quantized (ie. the empirical distributions).
We use the following notation:
• Let Xvk be the support of p(n|vk).
Then the mutual information can be estimated as following:
I(zj; vk)
=Eq(zj,vk) [log q(zj, vk) −log q(zj) −log p(vk)]
=Eq(zj,vk)
"
log
N
X
n=1
q(zj, vk, n) −log q(zj) −log p(vk)
#
=Ep(vk)p(n′|vk)q(zj|n′)
"
log
N
X
n=1
p(vk)p(n|vk)q(zj|n) −log q(zj) −log p(vk)
#
=Ep(vk)p(n′|vk)q(zj|n′)
"
log
N
X
n=1
1[n ∈Xvk]p(n|vk)q(zj|n)
#
+ Eq(zj) [−log q(zj)]
=Ep(vk)p(n′|vk)q(zj|n′)

log
X
n∈Xvk
q(zj|n)p(n|vk)

+ H(zj)
(S1)
where the expectation is to make sampling explicit.
To reduce variance, we perform stratiﬁed sampling over p(vk), and use 10, 000 samples from q(n, zk)
for each value of vk. To estimate H(zj) we sample from p(n)q(zj|n) and perform stratiﬁed sampling
over p(n). The computation time of our estimatation procedure depends on the dataset size but in
general can be done in a few minutes for the datasets in our experiments.
B.2 Normalization
It is known that when vk is discrete, then
I(zj; vk) = H(vk)
| {z } −H(vk|zj)
|
{z
}
≥0
≤H(vk)
(S2)
This bound is tight if the model can make H(vk|zj) zero, ie. there exist an invertible function between
zj and vk. On the other hand, if mutual information is not maximal, then we know it is because of a
high conditional entropy H(vk|zj). This suggests our metric is meaningful as it is measuring how
much information zj retains about vk regardless of the parameterization of their distributions.
7
C. ELBO TC-Decomposition
Proof of the decomposition in (2):
1
N
N
X
n=1
KL
 q(z|xn)||p(z)

= Ep(n)
h
KL
 q(z|n)||p(z)
i
= Ep(n)
h
Eq(z|n)

log q(z|n) −log p(z) + log q(z) −log q(z) + log
Y
j
q(zj) −log
Y
j
q(zj)
i
= Eq(z,n)
h
log q(z|n)
q(z)
i
+ Eq(z)
h
log
q(z)
Q
j q(zj)
i
+ Eq(z)

X
j
log q(zj)
p(zj)


= Eq(z,n)
h
log q(z|n)p(n)
q(z)p(n)
i
+ Eq(z)
h
log
q(z)
Q
j q(zj)
i
+
X
j
Eq(z)
h
log q(zj)
p(zj)
i
= Eq(z,n)
h
log q(z|n)p(n)
q(z)p(n)
i
+ Eq(z)
h
log
q(z)
Q
j q(zj)
i
+
X
j
Eq(zj)q(z\j|zj)
h
log q(zj)
p(zj)
i
= Eq(z,n)
h
log q(z|n)p(n)
q(z)p(n)
i
+ Eq(z)
h
log
q(z)
Q
j q(zj)
i
+
X
j
Eq(zj)
h
log q(zj)
p(zj)
i
= KL (q(z, n)||q(z)p(n))
|
{z
}
i
Index-Code MI
+ KL
 q(z)||
Y
j
q(zj)

|
{z
}
ii
Total Correlation
+
X
j
KL
 q(zj)||p(zj)

|
{z
}
iii Dimension-wise KL
C.1 Minibatch Weighted Sampling (MWS)
First, let BM = {n1, ..., nM} be a minibatch of M indices where each element is sampled i.i.d. from
p(n), so for any sampled batch instance BM, p(BM) = (1/N)M. Let r(BM|n) denote the probability
of a sampled minibatch where one of the elements is ﬁxed to be n and the rest are sampled i.i.d. from
p(n). This gives r(xM|n) = (1/N)M−1.
Eq(z) [log q(z)]
=Eq(z,n)

log En′∼p(n) [q(z|n′)]

=Eq(z,n)
"
log Ep(BM)
"
1
M
M
X
m=1
q(z|nm)
##
≥Eq(z,n)
"
log Er(BM|n)
"
p(BM)
r(BM|n)
1
M
M
X
m=1
q(z|nm)
##
=Eq(z,n)
"
log Er(BM|n)
"
1
NM
M
X
m=1
q(z|nm)
##
(S3)
The inequality is due to r having a support that is a subset of that of p. During training, when provided
with a minibatch of samples {n1, ..., nM}, we can use the estimator
Eq(z)[log q(z)] ≈1
M
M
X
i=1

log
M
X
j=1
q(z(ni)|nj) −log(NM)


(S4)
where z(ni) is a sample from q(z|ni).
C.2 Minibatch Stratiﬁed Sampling (MSS)
In this setting, we sample a minibatch of indices BM = {n1, . . . , nm} to estimate q(z) for some z
that was originally sampled from q(z|n∗) for a particular index n∗. We deﬁne p(BM) to be uniform
8
over all minibatches of size M. To sample from p(BM), we sample M indices from {1, . . . , N}
without replacement. Then the following expressions hold:
q(z) = Ep(n)[q(z|n)]
= Ep(BM)
"
1
M
M
X
m=1
q(z|nm)
#
= P(n∗∈BM)E
"
1
M
M
X
m=1
q(z|nm)
n∗∈BM
#
+ P(n∗̸∈BM)E
"
1
M
M
X
m=1
q(z|nm)
n∗̸∈BM
#
= M
N E
"
1
M
M
X
m=1
q(z|nm)
n∗∈BM
#
+ N −M
N
E
"
1
M
M
X
m=1
q(z|nm)
n∗̸∈BM
#
(S5)
During training, we sample a minibatch of size M without replacement that does not contain n∗.
We estimate the ﬁrst term using n∗and M −1 other samples, and the second term using the M
samples that are not n∗. One can also view this as sampling a minibatch of size M + 1 where n∗
is one of the elements, and let BM+1\{n∗} = ˆBM = {n1, . . . , nM} be the elements that are not
equal to n∗, then we can estimate the ﬁrst expectation using {n∗} ∪{n1, . . . , nM−1} and the second
expectation using {n1, . . . , nM}. This estimator can be written as:
f(z, n∗, ˆBM) = 1
N q(z|n∗) + 1
M
M−1
X
m=1
q(z|nm) + N −M
NM
q(z|nM)
(S6)
which is unbiased (ie. q(z) = E
h
f(z, n∗, ˆBM)
i
), and exact if M = N.
C.2.1 Stochastic Estimation
During training, we estimate each term of the decomposed ELBO, log p(x|z), log p(z), log q(z|x),
log q(z), and log QK
j=1 q(zj), where the last two terms are estimated using MSS. For convenience,
we use the same minibatch that was used to sample z to estimate these two terms.
Eq(z,n)[log q(z)] ≈
1
M + 1
M+1
X
i=1
log f(zi, ni, BM+1\{ni})
(S7)
Note that this estimator is a lower bound on Eq(z)[log q(z)] due to Jensen’s inequality,
Ep(BM+1)
"
1
M + 1
M+1
X
i=1
log f(zi, ni, BM+1\{ni})
#
=Ep(BM+1) [log f(zi, ni, BM+1\{ni})]
≤log Ep(BM+1) [f(zi, ni, BM+1\{ni})]
=Eq(z)[log q(z)]
(S8)
However, the bias goes to zero if M increases and the equality holds if M = N. (Note that this is
in terms of the empirical distribution p(n) used in our decomposition rather than the unknown data
distribution.)
C.2.2 Experiments
While MSS is an unbiased estimator of q(z), MWS is not. Moreover, neither of them is unbiased for
estimating log q(z) due to Jensen’s inequality. Take MSS as an example:
Ep(n) [log MSS(z)] ≤log Ep(n)[MSS(z)] = log q(z)
(S9)
We observe from preliminary experiments that using MSS results in performance similar to MWS.
9
2
4
6
8
10
150
125
100
75
50
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
MIG Metric
Comparison on 2D dSprites
-VAE
-TCVAE
-TCVAE (MSS)
2
4
6
8
10
1450
1440
1430
1420
1410
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
0.5
0.6
MIG Metric
Comparison on 3D Faces
-VAE
-TCVAE
-TCVAE (MSS)
Figure S7: MSS performs similarly to MWS.
10
C. Extra Ablation Experiments
We performed some ablation experiments using slight variants of β-TCVAE, but found no signiﬁcant
meaningful differences.
C.1 Removing Index-code MI (α = 0)
We show some preliminary experiments using α = 0 in (4). By removing the penalty on index-
code MI, the autoencoder can then place as much information as necessary into the latent variables.
However, we ﬁnd no signiﬁcant difference between setting α to 0 or to 1, and the setting is likely
empirically dataset-dependent. Further experiments use α = 1 so that it is a proper lower bound on
log p(x) and to avoid the extensive hyperparameter tuning of having to choose α. Note that works
claiming better representations can be obtained with low α [S6, S17] and moderate α [S3] both exist.
2
4
6
8
10
150
125
100
75
50
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
MIG Metric
Comparison on 2D dSprites
-VAE
-TCVAE
-TCVAE (no MI)
2
4
6
8
10
1450
1440
1430
1420
1410
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
0.5
0.6
MIG Metric
Comparison on 3D Faces
-VAE
-TCVAE
-TCVAE (no MI)
Figure S8: ELBO vs. Disentanglement plots showing β-TCVAE (4) but with α set to 0.
C.2 Factorial Normalizing Flow
We also performed experiments with a factorial normalizing ﬂow (FNF) as a ﬂexible prior. Using a
ﬂexible prior is conceptually similar to ignoring the dimension-wise KL term in (2) (ie. γ = 0 in (4)),
but empirically the slow updates for the normalizing ﬂow should help stabilize the aggregate posterior.
Each dimension is a normalizing ﬂow of depth 32, and the parameters are trained to maximize the
β-TCVAE objective. The FNF can ﬁt multi-modal distributions. From our preliminary experiments,
we found no signiﬁcant improvement from using a factorial Gaussian prior and so decided not to
include this in the paper.
11
2
4
6
8
10
150
125
100
75
50
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
MIG Metric
Comparison on 2D dSprites
2
4
6
8
10
1450
1440
1430
1420
1410
ELBO
2
4
6
8
10
0.1
0.2
0.3
0.4
0.5
0.6
MIG Metric
Comparison on 3D Faces
-VAE
-TCVAE
-TCVAE (FNF)
Figure S9: ELBO vs. Disentanglement plots showing the β-TCVAE with a factorial normalizing ﬂow
(FNF).
C.3 Effect of Batchsize
102
103
Batchsize
0.0
0.2
0.4
0.6
MIG Metric
Effect of Batchsize
Figure S10: We used a high batchsize to account for the bias in minibatch estimation. However, we
ﬁnd that lower batchsizes are still effective when using β-TCVAE, suggesting that a high batchsize
may not be necessary. This is run on the 3D faces dataset with β=6.
12
D. Comparison of Best Models
z7
z3
z6
pos
z1
scale
rotation
Ground Truth
Learned Latent Variables 
(a) Best β-TCVAE (MIG: 0.53)
z7
z1
z2
pos
z3
scale
rotation
Ground Truth
Learned Latent Variables 
(b) Best β-VAE (MIG: 0.50)
Figure S11: MIG is able to capture subtle differences. Relation between the learned variables
and the ground truth factors are plotted for the best β-TCVAE and β-VAE on the dSprites dataset
according to the MIG metric are shown. Each row corresponds to a ground truth factor and each
column to a latent variable. The plots show the relationship between the latent variable mean versus
the ground truth factor, with only active latent variables shown. For position, a color of blue indicates
a high value and red indicates a low value. The colored lines indicate object shape with red being
oval, green being square, and blue being heart. Interestingly, the latent variables for rotation has 2
peaks for oval and 4 peaks for square, suggesting that the models have produced a more compact
code by observing that the object is rendered the same for certain degrees of rotation.
E. Invariance to Hyperparameters
101
102
Hyperparameter L
0.5
0.6
0.7
0.8
0.9
1.0
Higgins Metric
Higgins Metric vs. L (Batchsize)
101
102
Hyperparameter L
0.4
0.5
0.6
0.7
0.8
0.9
Kim & Mnih Metric
Kim & Mnih Metric vs. L (Batchsize)
Figure S12: The distribution of each classiﬁer-based metric is shown to be extremely dependent
on the hyperparameter L. Each colored line is a different VAE trained with the unmodiﬁed ELBO
objective.
We believe that a metric should also be invariant to any hyperparameters. For instance, the existence
of hyperparameters in the prior metrics means that a different set of hyperparameter values can
result in different metric outputs. Additionally, even with a stable classiﬁer that always outputs the
same accuracy for a given dataset, the creation of a dataset for classiﬁer-based metrics can still be
problematic.
The aggregated inputs used by [S7] and [S8] depend on a batch size L that is difﬁcult to tune and
leads to inconsistent metric values. In fact, we empirically ﬁnd that these metrics are most informative
with a small L. Figure S12 plots the [S7] metric against L for 20 fully trained VAEs. As L increases,
the aggregated inputs become more quantized. Not only does this increase the accuracy of the metric,
13
but it also reduces the gap between models, making it hard to discriminate similarly performing
models. The relative ordering of models is also not preserved with different values.
F. MIG Traversal
To give some insight into what MIG is capturing, we show some β-TCVAE experiments with
scores near quantized values of MIG. In general, we ﬁnd that MIG gives low scores to entangled
representations when even just two variables are not axis-aligned. We ﬁnd that MIG shows a clearer
pattern for scoring position and scale, but less so for rotation. This is likely due to latent variables
having a low MI with rotation. In an unsupervised setting, certain ground truth rotation values are
impossible to differentiate (e.g. 0 and 180 for ovals and squares), so the latent variable simply learns
to map these to the same value. This is evident in the plots where latent variables describing rotation
are many-to-one. The existence of factors with redundant values may be one downside to using MIG
as a scoring mechanism, but such factors only appear in simple datasets such as dSprites.
Note that this type of plot does not show the whole picture. Speciﬁcally, only the mean of the latent
variables is shown, while the uncertainty of the latent variables is not. Mutual information computes
the reduction in uncertainty after observing one factor, so the uncertainty is important but cannot be
easily plotted. Some changes in MIG may be explained by a reduction in the uncertainty even though
the plots may look similar.
z1
z2
z3
z6
pos
z8
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.0169
z0
z4
z6
z7
z8
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.0214
Score near 0.0. Representations are extremely entangled.
14
z0
z5
z6
pos
z7
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.0988
z0
z1
z4
z6
pos
z8
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.1017
Score near 0.1. Representations are less entangled but fail to satisfy axis-alignment.
z0
z3
z4
pos
z7
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.2017
z0
z4
z5
z7
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.2029
Score near 0.2. Representations are still entangled, but some form is appearing.
15
z0
z2
z3
pos
z7
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.3155
z2
z5
z6
z7
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.3271
Score near 0.3. Representations look much more disentangled, with some nuances not being completely
disentangled.
z2
z3
z5
z6
pos
z8
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.4092
z0
z1
z2
pos
z6
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.4179
Score near 0.4. Representations appear to be axis-aligned but rotation is still entangled and position is not
perfect.
16
z2
z5
z7
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.4929
z1
z4
z8
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
MIG: 0.5033
Score near 0.5. Representations appear to be axis-aligned and disentangled. Higher scores are likely reducing
entropy (with latent variables appearing closer to a Dirac delta.) To fully match the ground truth, the latent
variables would have to be a mixture of Dirac deltas, but such variables would have a high dimension-wise KL
with a factorized Gaussian.
G. Disagreements Between Metrics
Higgins metric vs. MIG
0.0
0.1
0.2
0.3
0.4
0.5
MIG metric
0.5
0.6
0.7
0.8
0.9
1.0
Higgins metric
(a)
z0
z1
z3
z7
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
(b)
z0
z1
z5
z8
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
(c)
(d)
Figure S19: Entangled representations can have a relatively high Higgins metric while MIG
correctly scores it low. (a) The Higgins metric tends to be overly optimistic compared to the MIG
metric. (b, c) Relationships between the ground truth factors and the learned latent variables are
shown for the top two controversial models, which are shown as red dots. Each colored line indicates
a different shape (see Supplementary Materials). (d) Sample traversals for the two latent variables in
model (b) that both depend on rotation, which clearly mirror each other.
Before using the MIG metric, we ﬁrst show that it is in some ways superior to the [S7] metric. To ﬁnd
differences between these two metrics, we train 200 models of β-VAE with varying β and different
initializations.
Figure S19a shows each model as a single point based on the two metrics. In general, both metrics
agree on the most disentangled models; however, the MIG metric falls off very quickly comparatively.
17
In fact, the Higgins metric tends to output a inﬂated score due to its inability to detect subtle
differences and a lack of axis-alignment.
As an example, we can look at controversial models that are disagreed upon by the two metrics
(Figure S19b). The most controversial model is shown in Figure S19a as a red dot. While the MIG
metric only ranks this model as better than 26% of the models, the Higgins metric ranks it as better
than 75% of the models. By inspecting the relationship between the latent units and ground truth
factors, we see that only the scale factor seems to be disentangled (Figure S19b). The position factors
are not axis aligned, and there are two latent variables for rotation that appear to mirror each other
with only a very slight difference. The two rows in Figure S19d show traversals corresponding to
the two latent variables for rotation. We see clearly that they simply rotate in the opposite direction.
Since the Higgins metric does not enforce that only a single latent variable should inﬂuence each
factor, it mistakenly assigns a higher disentanglement score to this model. We note that many models
near the red dot in the ﬁgure exhibit similar behavior.
G.1 More Controversial Models
Each model can be ordered by either metric (MIG or Higgins) such that each model is assigned
a unique integer 1 −200. We deﬁne the most controversial model as maxα R(α; Higgins) −
R(α; MIG), where a higher rank implies more disentanglement. These are models that the Higgins
metric believes to be highly disentangled while MIG believes they are not. Figure S21 shows the top
5 most controversial models.
Higgins metric vs. MIG
0.0
0.1
0.2
0.3
0.4
0.5
MIG metric
0.5
0.6
0.7
0.8
0.9
1.0
Higgins metric
Figure S20: Models as colored dots are those shown in Figure S21.
z0
z1
z3
z7
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
(a) (26%, 75%)
z0
z1
z5
z8
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
(b) (11%, 57%)
z4
z5
pos
z7
scale
rotation
Ground Truth
Learned Latent Variables 
(c) (25%, 70%)
z1
z4
z5
z8
pos
z9
scale
rotation
Ground Truth
Learned Latent Variables 
(d) (34%, 78%)
z0
z2
z5
z7
pos
z8
scale
rotation
Ground Truth
Learned Latent Variables 
(e) (28%, 67%)
Figure S21: (a-e) The top 5 most controversial models. The brackets indicate the rank of models by
MIG and the Higgins metric. For instance, the most controversial model shown in (a) is ranked as
better than 75% of model by the Higgins metric, but MIG believes that it is only better than 25% of
models.
18
