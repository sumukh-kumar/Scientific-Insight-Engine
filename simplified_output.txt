Paper: Learning symmetries in datasets
Neural networks are powerful tools for generating data and discovering symmetries from data. In this work, we demonstrate using unsupervised learning to discover symmetry in real-world datasets with independent variables and an O(2 symmetry) constraint. Our technique can discover x1 and x2 symmetry from Gaussian random data, as in the case of electron-positron and proton-proton collisions. We also demonstrate using edge-group perturbation to reveal the position of x1 and x2 on a circle of fixed radius R, and use this to generate 104 events with R = 10. We illustrate these findings with examples from electron-positron and proton-proton collisions, as well as theoretical studies of condensed matter theories with symmetry.
Unsupervised learning is an exciting new technique for machine learning. However, it is often difficult to train a machine to discover the meaning of a data set if it contains symmetries or approximate symmetries. The state-of-the-art approaches for machine learning are remarkably different from those used for humans. In particular, machine learning can detect latent patterns in the data that are otherwise impossible to detect. In this work, we propose a generalisation of the best-fitting weights for a given data set, based on its intrinsic dimensionality. We show that the weights for the internal weights depend on the dimensionality of the data set, which can be estimated from the data.
The present work presents an approach to learning representations of three-dimensional domains that may be extended to other domains. First, we construct a circle whose root structure is  . Then, we divide the obtained data into two equal parts: x1 = R cos , x2 = R sin . (5.5) Since the data are embedded in R2, the shape of the circle must be intrinsically one-dimensional despite being embedded in R2.

Paper: Latent Space Symmetry Discovery
This paper presents the results of a special session on embedded learning in the context of the 41st International Conference on Machine Learning. The paper is aimed at advancing the field of Machine Learning by presenting state of the art results in the relevant area.
We report that stars in our solar system are exhibiting low-earth physics. The discovery of this system is a major step in our understanding of the inner workings of our solar system.

Paper: Lorentz group equivariant autoencoders
The datasets used in this study are publicly available on Zenodo [70], and the code for all models used in this paper can be found in a public repository [68]. This paper describes an efficient pipeline for the generation of 3-D biomineralization models from single-molecule droplets: a compressed version of the SAMPLE pipeline. This pipeline is based on the CALS model (SAMPLE-Probe) and is designed to be executed on any platform running either the CALS or the SAMPLE-Probe libraries. The code for all models used in this paper can be found in a public repository [68].

Paper: Symmetry meets AI
Mechanics is a general theory of dynamics that deals with the problem of predicting the future state of the system. Its generality is that in most cases, the system does not change much at all, and its predictions are generally accurate. We propose a methodology for generating such datasets. We rely on the idea that for a given system, the mean field error rate (i.e., the average error rate) is so low that almost all of its predictions are true. Moreover, we have uncovered a crucial point: In most physics problems, the problem of prediction has so far been solved by humans.

Paper: Better Latent Spaces for Better Autoencoders


Paper: Discovering physical concepts with neural networks
The problem of discovering the physical laws underlying experimental data has remained an open question in many domains of research. We have developed a set of methods to discover the physical laws underlying a set of data. These methods are based on the assumption that all data are independent of the data-driven process of discovery (see e.g. [10–13]). In this work, we use the Tensorflow li- brary as a model for discovering the physical laws. The following description explains the underlying data for a specific set of data.
The basic idea behind gradient descent is that, for a given input, the weights and biases of a network are initialized at random and then, through some computation, an average of the weights and biases is calculated and adapted to the input data. The resulting gradient is called gradient descent. In practice, gradient descent is often used instead of gradient over all training samples because the process of generating the gradient is much faster in real-world applications.
We develop a deep learning approach based on learning from data series: A simple variational model of the neural network. The model is based on the assumption that all information in the system is available to the data at the time it is needed. In the example system, the physical variables are only known in the specific case, and so can be classed as follows: [1]. Here we give a brief introduction to the problem of inference from data series.
We show that sciNet works well on all tested examples and that the optimum settings for  and  are then learned. Our algorithm is based on experiments; hence, the underlying physical mechanisms are not well understood. For a few parameters, a function , , and an optimum set of parameters are defined and analyzed. Then, the parameters are compared, and the results are compared. This is an iterative process; in other words, the choice of the optimal values is not a property of a physical system, but a choice we have to make.
A collection of deterministic data is usually regarded as a “simple” representation: The data are not correlated, and the information is not correlated with itself. An adequate description of the set would therefore be useful if all the deterministic information was escaped from the data. We define a minimal uncorrelated representation R as an uncorrelated (sufficient) representation with a mini-mal number of parameters |R|. This formalizes what we consider to be a “simple” representation of physical data.
Statistical inference relies on formulas that are expressible in the data in some abstract way (e.g., Fisher’s exact test, Bayesian statistics, or Poin-son’s test) and in a nonlinear way (e.g., Banach’s inequality, Green’s inequality, Snell’s inequality, etc.). These are called “factuals.” In mathematical terms, they are classes of variables that, in their concrete sense, are analogous to real numbers. In the sense of an analogy, the real numbers are analogous to the numbers in a probability distribution: These are the physical variables that are to be extracted from time series data series.
The current paper uses a neural network to make predictions about the phase transition in a phase-condensed matter system. The model is not restricted to this structure and the network can be extended to other systems with a similar physical context. For example, the network can be used to learn local symmetries in a condensed matter system. This type of network is very useful for making general inferences about properties of an interacting system. Neural networks have proved to be a powerful tool for making predictions in physics, especially in condensed matter systems.
Neural networks are the most successful machine-learning technique for solving a variety of tasks, including predicting the future of a computer, predicting the quantum nature of the world, and analyzing the physical cognition from sensory experience (fol- owing to the similarity of neural networks to human perception and understanding). For many applications, neural networks are expected to play a significant role in the development of artificial intelligence (see Appendix B for fur- ther discussions). In the future, however, the use of neural networks may be widely employed to investigate the nature of cognition, allowing for the devel- opment of new insights into how the human brain devel- ops physical intuition from observations [19–25].

Paper: Symmetry meets AI
Mechanics is a generalizable class of machine-learning algorithms that can deal with a variety of physical problems. As such, it offers a perfect platform for the generation of datasets that are as finely resolved as possible. In particular, it can be proved that Mechanics does not depend on the number of steps in the process (as in classical multilayers networks), so it is able to deal with very sparse data (as in the case of networks involving many thousands of points).

Paper: PELICAN: Permutation Equivariant and Lorentz Invariant or Covariant Aggregator Network for Particle Physics


Paper: Group Equivariant Convolutional Networks
A dataset of randomly rotated text inputs is published by Larochelle et al. in 2007 (in English). This dataset can be used to experiment with different hypotheses or to build models or experiments. In this paper, we use it to train a model based on the Rotated MNIST dataset.
The rate of learning was divided by 10 at epochs 50, 100 and 150, and training was continued for 300 epochs. The total learning rate was 30  10 = 100 y. The students studied for 10 y at each epoch, and thus the total learning rate was 30,000 y1.

Paper: Isolating Sources of Disentanglement in Variational Autoencoders
We propose a recursive neural network architecture to estimate the mean-mean correlation of a moving point model over a large number of observations. As each point in the network is a unique integer, the noise levels are reduced to a single integer. We measure the net effects of the changes in state of the network, and find a strong relation between total correlation and disentanglement.

